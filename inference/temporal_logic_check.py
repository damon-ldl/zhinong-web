import json
import re
import os
from pathlib import Path
import argparse
import datetime
from docx import Document

# ========================== 1. é…ç½®åŒº ==========================
API_KEY = "BearerCubJNNWPCQjooxiEbLGB:IgSlCSEPDRuFXHJlXlMw"
URL = "https://spark-api-open.xf-yun.com/v2/chat/completions"
INPUT_DOCUMENT_PATH = r"C:/Users/cassi/Desktop/bisai/word-master/è±«æ´›é˜³-å…°éƒ‘é•¿å¹²çº¿-CPY-0790-BFGDGS-ZZSYQFGS.docx"
OUTPUT_JSON_PATH = r"C:/Users/cassi/Desktop/bisai/word-master/temporal_logic_check.json"

# ä¸ºä¸å…¶å®ƒè„šæœ¬å¯¹é½çš„æ ·ä¾‹è·¯å¾„å¸¸é‡
SAMPLE_FILE_PATH = INPUT_DOCUMENT_PATH
SAMPLE_OUT_PATH = OUTPUT_JSON_PATH


# ========================== 2. åŠŸèƒ½å‡½æ•°åŒº ==========================


def extract_text_from_docx(docx_path):
    """æå–Wordæ–‡æ¡£ä¸­çš„çº¯æ–‡æœ¬å†…å®¹"""
    try:
        doc = Document(docx_path)
        full_text = ""
        for para in doc.paragraphs:
            text = para.text.strip()
            if text:
                full_text += text + "\n"

        for table in doc.tables:
            for row in table.rows:
                row_text = []
                for cell in row.cells:
                    cell_text = cell.text.strip()
                    if cell_text:
                        row_text.append(cell_text)
                if row_text:
                    full_text += "  ".join(row_text) + "\n"

        # ğŸ”§ è‡ªåŠ¨ä¿®æ­£è·¨è¡Œæ—¶é—´æ ‡è¯†ï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰
        # ä¾‹å¦‚ â€œè¯†åˆ«æ—¶é—´\n2024å¹´3æœˆâ€ â†’ â€œè¯†åˆ«æ—¶é—´ï¼š2024å¹´3æœˆâ€
        full_text = re.sub(r'(è¯†åˆ«æ—¶é—´)\s*\n\s*(\d{4}å¹´\d{1,2}æœˆ)', r'\1ï¼š\2', full_text)
        full_text = re.sub(r'(é£é™©è¯„ä»·æ—¶é—´)\s*\n\s*(\d{4}å¹´\d{1,2}æœˆ)', r'\1ï¼š\2', full_text)
        full_text = re.sub(r'(è¯„ä»·æ—¶é—´)\s*\n\s*(\d{4}å¹´\d{1,2}æœˆ)', r'\1ï¼š\2', full_text)
        return full_text
    except Exception as e:
        print(f"Wordæ–‡æ¡£æ–‡æœ¬æå–å¤±è´¥ï¼š{str(e)}")
        return ""


def extract_date_from_text(date_text):
    """ä»æ–‡æœ¬ä¸­æå–æ—¥æœŸ"""
    date_text = re.sub(r'[^\då¹´æœˆæ—¥./\-\s]', '', date_text)
    date_patterns = [
        r'(\d{4})[å¹´.-](\d{1,2})[æœˆ.-](\d{1,2})',
        r'(\d{4})[/.-](\d{1,2})[/.-](\d{1,2})',
        r'(\d{4})\s*å¹´\s*(\d{1,2})\s*æœˆ',
        r'(\d{1,2})[æœˆ.-](\d{1,2})[æ—¥ï¼Œ]?\s*(\d{4})',
    ]
    for pattern in date_patterns:
        match = re.search(pattern, date_text)
        if match:
            try:
                groups = match.groups()
                if len(groups) == 3:
                    if len(groups[0]) == 4:
                        year, month, day = int(groups[0]), int(groups[1]), int(groups[2])
                    else:
                        month, day, year = int(groups[0]), int(groups[1]), int(groups[2])
                elif len(groups) == 2:
                    year, month, day = int(groups[0]), int(groups[1]), 1
                if 2000 <= year <= 2035 and 1 <= month <= 12 and 1 <= day <= 31:
                    return datetime.date(year, month, day)
            except:
                continue
    return None


def extract_time_information(text):
    """æå–æ—¶é—´ä¿¡æ¯"""
    time_info = {
        'cover_dates': [],
        'identification_dates': [],
        'risk_assessment_dates': [],
        'other_dates': []
    }

    # å°é¢æ—¶é—´
    cover_patterns = [
        r'ç¼–åˆ¶æ—¶é—´[ï¼š:\s]*([^\n]+)',
        r'åˆ¶å®šæ—¶é—´[ï¼š:\s]*([^\n]+)',
        r'å‘å¸ƒæ—¶é—´[ï¼š:\s]*([^\n]+)',
        r'ç‰ˆæœ¬æ—¶é—´[ï¼š:\s]*([^\n]+)'
    ]
    for pattern in cover_patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            date_text = match.group(1).strip()
            extracted_date = extract_date_from_text(date_text)
            if extracted_date:
                time_info['cover_dates'].append({
                    'raw_text': date_text,
                    'parsed_date': extracted_date,
                    'context': match.group(0),
                    'position': match.span()
                })

    # è¯†åˆ«æ—¶é—´
    identification_patterns = [
        r'è¯†åˆ«æ—¶é—´[ï¼š:\s]*([^\n]*)',
        r'è¯†åˆ«æ—¥æœŸ[ï¼š:\s]*([^\n]*)',
        r'å®Œæˆè¯†åˆ«.*?(\d{4}å¹´\d{1,2}æœˆ)',
    ]
    for pattern in identification_patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            date_text = match.group(1).strip()
            extracted_date = extract_date_from_text(date_text)
            if extracted_date:
                time_info['identification_dates'].append({
                    'raw_text': date_text,
                    'parsed_date': extracted_date,
                    'context': match.group(0),
                    'position': match.span()
                })

    # é£é™©è¯„ä»·æ—¶é—´
    assessment_patterns = [
        r'é£é™©è¯„ä»·æ—¶é—´[ï¼š:\s]*([^\n]*)',
        r'é£é™©è¯„ä»·æ—¥æœŸ[ï¼š:\s]*([^\n]*)',
        r'è¯„ä»·æ—¶é—´[ï¼š:\s]*([^\n]*)',
        r'å®Œæˆè¯„ä»·.*?(\d{4}å¹´\d{1,2}æœˆ)',
    ]
    for pattern in assessment_patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            date_text = match.group(1).strip()
            extracted_date = extract_date_from_text(date_text)
            if extracted_date:
                time_info['risk_assessment_dates'].append({
                    'raw_text': date_text,
                    'parsed_date': extracted_date,
                    'context': match.group(0),
                    'position': match.span()
                })

    # âœ… æ–°å¢ï¼šåªä¿ç•™ç¬¬ä¸€ä¸ªé£é™©è¯„ä»·æ—¶é—´ï¼Œé¿å…é‡å¤è¯†åˆ«
    if len(time_info['risk_assessment_dates']) > 1:
        # print(f"âš ï¸ æ£€æµ‹åˆ°å¤šä¸ªé£é™©è¯„ä»·æ—¶é—´ï¼ˆå…±{len(time_info['risk_assessment_dates'])}ä¸ªï¼‰ï¼Œä»…ä¿ç•™ç¬¬ä¸€ä¸ªã€‚")
        time_info['risk_assessment_dates'] = [time_info['risk_assessment_dates'][0]]

    return time_info


def build_time_snippets(text, time_info, context_window=120):
    """æ ¹æ®è¯†åˆ«åˆ°çš„æ—¶é—´ä½ç½®ä¿¡æ¯ï¼Œä»åŸæ–‡ä¸­åˆ‡ç‰‡ç”Ÿæˆä¸Šä¸‹æ–‡ç‰‡æ®µã€‚"""
    snippets = []
    for key in ['cover_dates', 'identification_dates', 'risk_assessment_dates']:
        for item in time_info.get(key, []):
            start, end = item.get('position', (0, 0))
            s = max(0, start - context_window)
            e = min(len(text), end + context_window)
            snippet = text[s:e].strip()
            if snippet and snippet not in snippets:
                snippets.append(snippet)
    # è‹¥æ²¡æœ‰å®šä½åˆ°å…·ä½“ä½ç½®ï¼Œåˆ™å›é€€ä½¿ç”¨å‰ N å­—ç¬¦
    if not snippets and text:
        snippets.append(text[: max(300, min(800, len(text)))] )
    return snippets


def format_year_month(d):
    """å°†æ—¥æœŸæ ¼å¼åŒ–ä¸ºYYYYå¹´Mæœˆã€‚"""
    return f"{d.year}å¹´{d.month}æœˆ"


def build_canonical_time_line(time_info):
    """æ„é€ è§„èŒƒå•è¡Œæ–‡æœ¬ï¼šå°é¢ç¼–åˆ¶æ—¶é—´ï¼šxxï¼Œé«˜åæœåŒºè¯†åˆ«æ—¶é—´ï¼šxxï¼Œé£é™©è¯„ä»·æ—¶é—´ï¼šxx"""
    cover = time_info.get('cover_dates', [])
    ident = time_info.get('identification_dates', [])
    assess = time_info.get('risk_assessment_dates', [])

    cover_str = format_year_month(cover[0]['parsed_date']) if cover else 'xx'
    ident_str = format_year_month(ident[0]['parsed_date']) if ident else 'xx'
    assess_str = format_year_month(assess[0]['parsed_date']) if assess else 'xx'

    return f"å°é¢ç¼–åˆ¶æ—¶é—´ï¼š{cover_str}ï¼Œé«˜åæœåŒºè¯†åˆ«æ—¶é—´ï¼š{ident_str}ï¼Œé£é™©è¯„ä»·æ—¶é—´ï¼š{assess_str}"


def get_temporal_negative_prompt(canonical_line, context_text, variant="order_conflict"):
    """æ„é€ â€œæ—¶é—´é€»è¾‘å¯¹æŠ—æ ·æœ¬â€æç¤ºè¯ï¼šåŒ…å«è§„èŒƒå•è¡Œä¸åŸå§‹ä¸Šä¸‹æ–‡ç‰‡æ®µä¸¤éƒ¨åˆ†ã€‚"""
    variant_to_desc = {
        # è®©â€œé£é™©è¯„ä»·æ—¶é—´â€æœªæ™šäºâ€œé«˜åæœåŒºè¯†åˆ«æ—¶é—´â€ï¼Œè¿èƒŒæ—¶é—´é€»è¾‘
        'order_conflict': (
            "ä»»åŠ¡ï¼šç”Ÿæˆâ€˜æ—¶é—´å…ˆåé¡ºåºä¸åˆç†â€™çš„å¯¹æŠ—æ ·æœ¬ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. åŸå§‹æ–‡æœ¬ä¸ºè§„èŒƒå•è¡Œï¼šâ€˜å°é¢ç¼–åˆ¶æ—¶é—´ï¼šYYYYå¹´Mæœˆï¼Œé«˜åæœåŒºè¯†åˆ«æ—¶é—´ï¼šYYYYå¹´Mæœˆï¼Œé£é™©è¯„ä»·æ—¶é—´ï¼šYYYYå¹´Mæœˆâ€™ã€‚ä¿æŒç»“æ„ä¸å­—æ®µåä¸å˜ã€‚\n"
            "2. å°½é‡ä¸æ”¹å¹´ä»½ï¼ˆYYYYï¼‰ï¼Œä¼˜å…ˆä»…æ”¹æœˆä»½/æ—¥ï¼Œä½¿â€˜é£é™©è¯„ä»·æ—¶é—´â€™æœªæ™šäºâ€˜é«˜åæœåŒºè¯†åˆ«æ—¶é—´â€™ï¼ˆå¯åŒæœˆæˆ–æ›´æ—©ï¼‰ã€‚\n"
            "3. ä¸ä¿®æ”¹éæ—¶é—´æ–‡æœ¬ä¸å®ä½“åç§°ã€‚\n"
            "åŸå§‹æ–‡æœ¬ï¼ˆè§„èŒƒå•è¡Œï¼‰ï¼š\n---\n{canonical}\n---\n\nåŸå§‹ä¸Šä¸‹æ–‡ç‰‡æ®µï¼š\n---\n{context}\n---\n"
        ),
        # å¹´ä»½ä¸ä¸€è‡´ï¼ˆä»…ä¿®æ”¹å¹´ä»½ï¼‰ï¼ŒæŒ‡å®šä¸ä¸€è‡´çš„ä¸¤ç±»æ—¶é—´
        'year_inconsistency_cover_identification': (
            "ä»»åŠ¡ï¼šç”Ÿæˆâ€˜å¹´ä»½ä¸ä¸€è‡´ï¼ˆå°é¢ç¼–åˆ¶æ—¶é—´ vs é«˜åæœåŒºè¯†åˆ«æ—¶é—´ï¼‰â€™çš„å¯¹æŠ—æ ·æœ¬ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. åŸå§‹æ–‡æœ¬ä¸ºè§„èŒƒå•è¡Œï¼Œä¿æŒç»“æ„ä¸å­—æ®µåä¸å˜ã€‚\n"
            "2. ä»…ä¿®æ”¹å¹´ä»½ï¼ˆYYYYï¼‰ï¼Œä½¿â€˜å°é¢ç¼–åˆ¶æ—¶é—´â€™ä¸â€˜é«˜åæœåŒºè¯†åˆ«æ—¶é—´â€™å¹´ä»½ä¸åŒï¼ˆå¦‚å°é¢2023ï¼Œè¯†åˆ«2024ï¼‰ã€‚\n"
            "3. æœˆä»½å°½é‡ä¿æŒä¸å˜ï¼›ä¸ä¿®æ”¹éæ—¶é—´æ–‡æœ¬ã€‚\n"
            "4. è¾“å‡ºä»…ä¸ºä¿®æ”¹åçš„æ–‡æœ¬ã€‚\n"
            "åŸå§‹æ–‡æœ¬ï¼ˆè§„èŒƒå•è¡Œï¼‰ï¼š\n---\n{canonical}\n---\n\nåŸå§‹ä¸Šä¸‹æ–‡ç‰‡æ®µï¼š\n---\n{context}\n---\n"
        ),
        'year_inconsistency_cover_assessment': (
            "ä»»åŠ¡ï¼šç”Ÿæˆâ€˜å¹´ä»½ä¸ä¸€è‡´ï¼ˆå°é¢ç¼–åˆ¶æ—¶é—´ vs é£é™©è¯„ä»·æ—¶é—´ï¼‰â€™çš„å¯¹æŠ—æ ·æœ¬ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. åŸå§‹æ–‡æœ¬ä¸ºè§„èŒƒå•è¡Œï¼Œä¿æŒç»“æ„ä¸å­—æ®µåä¸å˜ã€‚\n"
            "2. ä»…ä¿®æ”¹å¹´ä»½ï¼ˆYYYYï¼‰ï¼Œä½¿â€˜å°é¢ç¼–åˆ¶æ—¶é—´â€™ä¸â€˜é£é™©è¯„ä»·æ—¶é—´â€™å¹´ä»½ä¸åŒã€‚\n"
            "3. æœˆä»½å°½é‡ä¿æŒä¸å˜ï¼›ä¸ä¿®æ”¹éæ—¶é—´æ–‡æœ¬ã€‚\n"
            "4. è¾“å‡ºä»…ä¸ºä¿®æ”¹åçš„æ–‡æœ¬ã€‚\n"
            "åŸå§‹æ–‡æœ¬ï¼ˆè§„èŒƒå•è¡Œï¼‰ï¼š\n---\n{canonical}\n---\n\nåŸå§‹ä¸Šä¸‹æ–‡ç‰‡æ®µï¼š\n---\n{context}\n---\n"
        ),
        'year_inconsistency_identification_assessment': (
            "ä»»åŠ¡ï¼šç”Ÿæˆâ€˜å¹´ä»½ä¸ä¸€è‡´ï¼ˆé«˜åæœåŒºè¯†åˆ«æ—¶é—´ vs é£é™©è¯„ä»·æ—¶é—´ï¼‰â€™çš„å¯¹æŠ—æ ·æœ¬ã€‚\n"
            "è¦æ±‚ï¼š\n"
            "1. åŸå§‹æ–‡æœ¬ä¸ºè§„èŒƒå•è¡Œï¼Œä¿æŒç»“æ„ä¸å­—æ®µåä¸å˜ã€‚\n"
            "2. ä»…ä¿®æ”¹å¹´ä»½ï¼ˆYYYYï¼‰ï¼Œä½¿â€˜é«˜åæœåŒºè¯†åˆ«æ—¶é—´â€™ä¸â€˜é£é™©è¯„ä»·æ—¶é—´â€™å¹´ä»½ä¸åŒã€‚\n"
            "3. æœˆä»½å°½é‡ä¿æŒä¸å˜ï¼›ä¸ä¿®æ”¹éæ—¶é—´æ–‡æœ¬ã€‚\n"
            "4. è¾“å‡ºä»…ä¸ºä¿®æ”¹åçš„æ–‡æœ¬ã€‚\n"
            "åŸå§‹æ–‡æœ¬ï¼ˆè§„èŒƒå•è¡Œï¼‰ï¼š\n---\n{canonical}\n---\n\nåŸå§‹ä¸Šä¸‹æ–‡ç‰‡æ®µï¼š\n---\n{context}\n---\n"
        ),
    }
    head = variant_to_desc.get(variant, variant_to_desc['order_conflict'])
    # ç”¨å…·ä½“åŸå§‹æ–‡æœ¬å¡«å……æ¨¡æ¿
    prompt_filled = head.format(canonical=canonical_line or "", context=context_text or "")
    return f"{prompt_filled}\nè¾“å‡ºï¼šä»…è¿”å›ä¿®æ”¹åçš„æ–‡æœ¬ï¼Œæ— éœ€é¢å¤–è§£é‡Šã€‚"


def check_temporal_logic(time_info):
    """æ£€æŸ¥æ—¶é—´é€»è¾‘ä¸€è‡´æ€§"""
    logic_results = {
        'year_consistency': True,
        'temporal_order_correct': True,
        'is_correct': True,
        'issues': [],
        'time_analysis': {}
    }

    all_dates = {
        'cover': [d['parsed_date'] for d in time_info['cover_dates']],
        'identification': [d['parsed_date'] for d in time_info['identification_dates']],
        'risk_assessment': [d['parsed_date'] for d in time_info['risk_assessment_dates']]
    }

    logic_results['time_analysis'] = {
        'cover_dates_count': len(all_dates['cover']),
        'identification_dates_count': len(all_dates['identification']),
        'risk_assessment_dates_count': len(all_dates['risk_assessment'])
    }

    # å¹´ä»½ä¸€è‡´æ€§ï¼ˆç»†åˆ†ä¸‰ç±»ä¸¤ä¸¤ä¸ä¸€è‡´ï¼‰
    cover_years = set(d.year for d in all_dates['cover'])
    identification_years = set(d.year for d in all_dates['identification'])
    risk_years = set(d.year for d in all_dates['risk_assessment'])

    # ä»»æ„ä¸¤ç±»å‡å­˜åœ¨æ—¶è¿›è¡Œå¯¹æ¯”
    if cover_years and identification_years and cover_years != identification_years:
        logic_results['year_consistency'] = False
        logic_results['is_correct'] = False
        logic_results['issues'].append(
            f"å¹´ä»½ä¸ä¸€è‡´ï¼ˆå°é¢ç¼–åˆ¶æ—¶é—´ vs é«˜åæœåŒºè¯†åˆ«æ—¶é—´ï¼‰ï¼šå°é¢{sorted(cover_years)}ï¼Œè¯†åˆ«{sorted(identification_years)}"
        )
    if cover_years and risk_years and cover_years != risk_years:
        logic_results['year_consistency'] = False
        logic_results['is_correct'] = False
        logic_results['issues'].append(
            f"å¹´ä»½ä¸ä¸€è‡´ï¼ˆå°é¢ç¼–åˆ¶æ—¶é—´ vs é£é™©è¯„ä»·æ—¶é—´ï¼‰ï¼šå°é¢{sorted(cover_years)}ï¼Œè¯„ä»·{sorted(risk_years)}"
        )
    if identification_years and risk_years and identification_years != risk_years:
        logic_results['year_consistency'] = False
        logic_results['is_correct'] = False
        logic_results['issues'].append(
            f"å¹´ä»½ä¸ä¸€è‡´ï¼ˆé«˜åæœåŒºè¯†åˆ«æ—¶é—´ vs é£é™©è¯„ä»·æ—¶é—´ï¼‰ï¼šè¯†åˆ«{sorted(identification_years)}ï¼Œè¯„ä»·{sorted(risk_years)}"
        )

    # æ—¶é—´å…ˆåé€»è¾‘
    if all_dates['identification'] and all_dates['risk_assessment']:
        if min(all_dates['risk_assessment']) <= max(all_dates['identification']):
            logic_results['temporal_order_correct'] = False
            logic_results['is_correct'] = False
            logic_results['issues'].append("é£é™©è¯„ä»·æ—¶é—´æœªæ™šäºé«˜åæœåŒºè¯†åˆ«æ—¶é—´")

    # æ—¶é—´åˆç†æ€§
    current_date = datetime.date.today()
    for date_type, dates in all_dates.items():
        for d in dates:
            if d > current_date:
                logic_results['issues'].append(f"{date_type}æ—¶é—´({d})ä¸ºæœªæ¥æ—¶é—´")
                logic_results['is_correct'] = False

    # æ—¶é—´ç¼ºå¤±
    if not all_dates['cover']:
        logic_results['issues'].append("ç¼ºå°‘å°é¢ç¼–åˆ¶æ—¶é—´")
        logic_results['is_correct'] = False
    if not all_dates['identification']:
        logic_results['issues'].append("ç¼ºå°‘é«˜åæœåŒºè¯†åˆ«æ—¶é—´")
        logic_results['is_correct'] = False
    if not all_dates['risk_assessment']:
        logic_results['issues'].append("ç¼ºå°‘é£é™©è¯„ä»·æ—¶é—´")
        logic_results['is_correct'] = False

    return logic_results


# ========================== æŠ¥å‘Šç”Ÿæˆ ==========================
def build_temporal_report(time_info, logic) -> str:
    lines = []
    # æ¦‚è¦
    lines.append("æ—¶é—´é€»è¾‘æ£€æŸ¥ï¼š")
    lines.append("")

    # è§„èŒƒå•è¡Œ
    def format_year_month(d):
        return f"{d.year}å¹´{d.month}æœˆ"
    cover = time_info.get('cover_dates', [])
    ident = time_info.get('identification_dates', [])
    assess = time_info.get('risk_assessment_dates', [])
    cover_str = format_year_month(cover[0]['parsed_date']) if cover else 'xx'
    ident_str = format_year_month(ident[0]['parsed_date']) if ident else 'xx'
    assess_str = format_year_month(assess[0]['parsed_date']) if assess else 'xx'
    lines.append(f"è§„èŒƒæ±‡æ€»ï¼šå°é¢ç¼–åˆ¶æ—¶é—´ï¼š{cover_str}ï¼Œé«˜åæœåŒºè¯†åˆ«æ—¶é—´ï¼š{ident_str}ï¼Œé£é™©è¯„ä»·æ—¶é—´ï¼š{assess_str}")
    lines.append("")

    # æ˜ç»†
    lines.append("æ—¶é—´ä¿¡æ¯æ˜ç»†ï¼š")
    for key, items in time_info.items():
        if key not in ['cover_dates', 'identification_dates', 'risk_assessment_dates']:
            continue
        cn = {
            'cover_dates': 'å°é¢ç¼–åˆ¶æ—¶é—´',
            'identification_dates': 'é«˜åæœåŒºè¯†åˆ«æ—¶é—´',
            'risk_assessment_dates': 'é£é™©è¯„ä»·æ—¶é—´',
        }.get(key, key)
        lines.append(f"- {cn}ï¼ˆ{len(items)} ä¸ªï¼‰ï¼š")
        for i, item in enumerate(items, 1):
            lines.append(f"  {i}. {item['context']} -> {item['parsed_date']}")
    lines.append("")

    # ç»“æœ
    if logic.get('issues'):
        lines.append("ç»“è®ºï¼šå­˜åœ¨é—®é¢˜")
        for idx, iss in enumerate(logic['issues'], 1):
            lines.append(f"- é—®é¢˜{idx}ï¼š{iss}")
    else:
        lines.append("ç»“è®ºï¼šæ— é—®é¢˜")

    return "\n".join(lines)


# ========================== 3. CLI å¯¹é½ï¼ˆä¸å…¶å®ƒè„šæœ¬ä¸€è‡´ï¼‰ ==========================
def _is_valid_docx_path(p: Path) -> bool:
    return p.suffix.lower() == ".docx" and not p.name.startswith("~$") and p.exists()


def _auto_discover_docx() -> Path | None:
    cwd = Path.cwd()
    candidates = []
    wm = cwd / "word-master"
    if wm.exists():
        for p in wm.rglob("*.docx"):
            if _is_valid_docx_path(p):
                candidates.append(p)
    for p in cwd.glob("*.docx"):
        if _is_valid_docx_path(p):
            candidates.append(p)
    if not candidates:
        for p in cwd.rglob("*.docx"):
            if _is_valid_docx_path(p):
                candidates.append(p)
                break
    return candidates[0] if candidates else None


def parse_args():
    ap = argparse.ArgumentParser(description="æ—¶é—´é€»è¾‘æ£€æŸ¥ä¸å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ")
    ap.add_argument("--input", required=False, default=None, help="è¾“å…¥ DOCX æ–‡ä»¶è·¯å¾„ï¼›ä¸æä¾›åˆ™ä¼˜å…ˆä½¿ç”¨æ ·ä¾‹ï¼Œå¦åˆ™è‡ªåŠ¨æŸ¥æ‰¾")
    ap.add_argument("--out_path", required=False, default=None, help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    return ap.parse_args()


# ========================== ä¸»ç¨‹åº ==========================
if __name__ == '__main__':
    args = parse_args()
    chosen = None
    used_sample = False
    if args.input:
        p = Path(args.input)
        chosen = p if _is_valid_docx_path(p) else None
    else:
        sample = Path(SAMPLE_FILE_PATH)
        if _is_valid_docx_path(sample):
            chosen = sample
            used_sample = True
        else:
            chosen = _auto_discover_docx()

    if not chosen:
        print("æœªæ‰¾åˆ°å¯å¤„ç†çš„ DOCX æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ --input æŒ‡å®šæ–‡ä»¶è·¯å¾„ã€‚")
        raise SystemExit(0)

    text = extract_text_from_docx(str(chosen))

    time_info = extract_time_information(text)

    logic = check_temporal_logic(time_info)

    # =============== ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Šï¼ˆä¸è¿›è¡Œæ­£è´Ÿæ ·æœ¬ç”Ÿæˆï¼‰ ===============
    report = build_temporal_report(time_info, logic)

    # æŒä¹…åŒ–è¾“å‡ºï¼šä¼˜å…ˆä½¿ç”¨ --out_pathï¼Œå¦åˆ™æ ·ä¾‹è·¯å¾„ï¼›å†™å…¥ä¸ºçº¯æ–‡æœ¬
    out_file = args.out_path if getattr(args, 'out_path', None) else (SAMPLE_OUT_PATH if used_sample else None)
    if out_file:
        try:
            out_dir = os.path.dirname(out_file)
            if out_dir and not os.path.exists(out_dir):
                os.makedirs(out_dir)
            with open(out_file, "w", encoding="utf-8") as f:
                f.write(report)
        except Exception as e:
            print(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥ï¼š{e}")
    print(report)
